{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2fdb07ec-71c3-4226-88d3-8248bb7a04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa631cde-dbad-40b1-acf2-45f91e7f17f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\gcv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\gcv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\gcv\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from xgboost) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "475aefb7-fe11-4b1b-a3d1-040eb97a1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import sys\n",
    "import logging\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils import PipelineUtils\n",
    "from data_processing import DataProcessor\n",
    "from clustering import CustomerClustering\n",
    "from model_training import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14778470-00b0-4202-83f8-e04648c860af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:17:13,700 - utils - INFO - Logging setup completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize utilities\n",
    "utils = PipelineUtils()\n",
    "utils.setup_logging(log_level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01f4c7d6-5472-4009-b591-deceff3effdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "827c6d4b-6b41-4768-8391-ddc2de393dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8026093c-7450-4d5c-b20f-bfbb745a782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:02,504 - data_processing - INFO - Loading data from local file: data/marketing_campaign.csv\n",
      "2025-09-22 02:29:02,517 - data_processing - INFO - Data loaded successfully. Shape: (2240, 29)\n",
      "2025-09-22 02:29:02,518 - data_processing - INFO - Starting data cleaning...\n",
      "2025-09-22 02:29:02,518 - data_processing - INFO - Original data shape: (2240, 29)\n",
      "2025-09-22 02:29:02,521 - data_processing - INFO - Original missing values:\n",
      "ID                      0\n",
      "Year_Birth              0\n",
      "Education               0\n",
      "Marital_Status          0\n",
      "Income                 24\n",
      "Kidhome                 0\n",
      "Teenhome                0\n",
      "Dt_Customer             0\n",
      "Recency                 0\n",
      "MntWines                0\n",
      "MntFruits               0\n",
      "MntMeatProducts         0\n",
      "MntFishProducts         0\n",
      "MntSweetProducts        0\n",
      "MntGoldProds            0\n",
      "NumDealsPurchases       0\n",
      "NumWebPurchases         0\n",
      "NumCatalogPurchases     0\n",
      "NumStorePurchases       0\n",
      "NumWebVisitsMonth       0\n",
      "AcceptedCmp3            0\n",
      "AcceptedCmp4            0\n",
      "AcceptedCmp5            0\n",
      "AcceptedCmp1            0\n",
      "AcceptedCmp2            0\n",
      "Complain                0\n",
      "Z_CostContact           0\n",
      "Z_Revenue               0\n",
      "Response                0\n",
      "dtype: int64\n",
      "2025-09-22 02:29:02,524 - data_processing - INFO - After dropping missing IDs: (2240, 29)\n",
      "2025-09-22 02:29:02,527 - data_processing - INFO - Filled missing Income with median: 51381.5\n",
      "2025-09-22 02:29:02,539 - data_processing - INFO - Converted Dt_Customer to datetime\n",
      "2025-09-22 02:29:02,566 - data_processing - INFO - Final cleaned data shape: (2240, 29)\n",
      "2025-09-22 02:29:02,571 - data_processing - INFO - Final missing values:\n",
      "ID                     0\n",
      "Year_Birth             0\n",
      "Education              0\n",
      "Marital_Status         0\n",
      "Income                 0\n",
      "Kidhome                0\n",
      "Teenhome               0\n",
      "Dt_Customer            0\n",
      "Recency                0\n",
      "MntWines               0\n",
      "MntFruits              0\n",
      "MntMeatProducts        0\n",
      "MntFishProducts        0\n",
      "MntSweetProducts       0\n",
      "MntGoldProds           0\n",
      "NumDealsPurchases      0\n",
      "NumWebPurchases        0\n",
      "NumCatalogPurchases    0\n",
      "NumStorePurchases      0\n",
      "NumWebVisitsMonth      0\n",
      "AcceptedCmp3           0\n",
      "AcceptedCmp4           0\n",
      "AcceptedCmp5           0\n",
      "AcceptedCmp1           0\n",
      "AcceptedCmp2           0\n",
      "Complain               0\n",
      "Z_CostContact          0\n",
      "Z_Revenue              0\n",
      "Response               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory changed to: C:\\Users\\GCV\\churn_prediction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change the current working directory to the project's root.\n",
    "# This is a robust way to ensure all file paths are correct.\n",
    "try:\n",
    "    os.chdir(r'C:\\Users\\GCV\\churn_prediction')\n",
    "    print(f\"Working directory changed to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Could not change directory. Please check the path.\")\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Now, your relative paths will work correctly.\n",
    "from src.data_processing import DataProcessor\n",
    "\n",
    "\n",
    "df = processor.load_data_from_local('data/marketing_campaign.csv')\n",
    "df_clean = processor.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "70c8fc60-b58d-498f-9543-68fb223dde68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:03,475 - data_processing - INFO - Creating RFM features...\n",
      "2025-09-22 02:29:03,477 - data_processing - INFO - Snapshot date for RFM: 2014-06-30 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 2: Creating RFM features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:03,695 - data_processing - INFO - RFM features created. Shape: (2240, 4)\n",
      "2025-09-22 02:29:03,716 - data_processing - INFO - RFM stats:\n",
      "                 ID      Recency  Frequency       Monetary\n",
      "count   2240.000000  2240.000000     2240.0    2240.000000\n",
      "mean    5592.159821   354.582143        1.0   52237.975446\n",
      "std     3246.662198   202.122512        0.0   25037.955891\n",
      "min        0.000000     1.000000        1.0    1730.000000\n",
      "25%     2828.250000   181.750000        1.0   35538.750000\n",
      "50%     5458.500000   356.500000        1.0   51381.500000\n",
      "75%     8427.750000   530.000000        1.0   68289.750000\n",
      "max    11191.000000   700.000000        1.0  666666.000000\n"
     ]
    }
   ],
   "source": [
    "# Create RFM features\n",
    "print(\" STEP 2: Creating RFM features...\")\n",
    "rfm_df = processor.create_rfm_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d350632d-a67b-4d31-a721-3d8418a32b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:04,657 - data_processing - INFO - Creating churn label with threshold: 90 days\n",
      "2025-09-22 02:29:04,661 - data_processing - INFO - Churn distribution:\n",
      "churn\n",
      "1    1960\n",
      "0     280\n",
      "Name: count, dtype: int64\n",
      "2025-09-22 02:29:04,663 - data_processing - INFO - Churn rate: 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 3: Creating churn labels...\n",
      " Data processing completed!\n",
      "Final dataset shape: (2240, 5)\n",
      "Churn distribution:\n",
      "churn\n",
      "1    1960\n",
      "0     280\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create churn labels\n",
    "print(\" STEP 3: Creating churn labels...\")\n",
    "rfm_with_churn = processor.create_churn_label(recency_threshold=90)\n",
    "print(\" Data processing completed!\")\n",
    "print(f\"Final dataset shape: {rfm_with_churn.shape}\")\n",
    "print(f\"Churn distribution:\\n{rfm_with_churn['churn'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f12f511b-9cef-43f1-bef2-b84367f0800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clustering\n",
    "clustering = CustomerClustering(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14490dad-44ce-454e-951c-9003018ad768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:21,441 - clustering - INFO - Preparing data for clustering...\n",
      "2025-09-22 02:29:21,443 - clustering - INFO - Data shape: (2240, 3)\n",
      "2025-09-22 02:29:21,446 - clustering - INFO - Features scaled using StandardScaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 4: Preparing data for clustering...\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for clustering\n",
    "print(\" STEP 4: Preparing data for clustering...\")\n",
    "X_scaled = clustering.prepare_data(rfm_df, scale_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "35108c70-bdd7-4142-80d2-9372186b6de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:23,840 - clustering - INFO - Finding optimal number of clusters...\n",
      "2025-09-22 02:29:23,842 - clustering - INFO - Testing k = 2...\n",
      "2025-09-22 02:29:23,999 - clustering - INFO - Testing k = 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 5: Finding optimal clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:24,180 - clustering - INFO - Testing k = 4...\n",
      "2025-09-22 02:29:24,337 - clustering - INFO - Testing k = 5...\n",
      "2025-09-22 02:29:24,503 - clustering - INFO - Testing k = 6...\n",
      "2025-09-22 02:29:24,666 - clustering - INFO - Testing k = 7...\n",
      "2025-09-22 02:29:24,837 - clustering - INFO - Testing k = 8...\n",
      "2025-09-22 02:29:25,017 - clustering - INFO - Optimal number of clusters: 2\n",
      "2025-09-22 02:29:25,018 - clustering - INFO - Best silhouette score: 0.3904\n"
     ]
    }
   ],
   "source": [
    "# Find optimal number of clusters\n",
    "print(\" STEP 5: Finding optimal clusters...\")\n",
    "optimal_info = clustering.find_optimal_clusters(X_scaled, max_clusters=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dbb0a53c-e6d4-4a91-8b32-4d49d100451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:27,004 - clustering - INFO - Performing K-means clustering with 2 clusters...\n",
      "2025-09-22 02:29:27,144 - clustering - INFO - Clustering completed. Silhouette score: 0.3904\n",
      "2025-09-22 02:29:27,146 - clustering - INFO - Cluster distribution:\n",
      "1    1135\n",
      "0    1105\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 6: Performing clustering...\n"
     ]
    }
   ],
   "source": [
    "# Perform clustering with optimal k\n",
    "print(\" STEP 6: Performing clustering...\")\n",
    "cluster_labels = clustering.perform_clustering(X_scaled, n_clusters=optimal_info['optimal_k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac3b8b54-c0e4-49c6-8c62-c3e9ca2094c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:29:30,504 - clustering - INFO - Analyzing cluster characteristics...\n",
      "2025-09-22 02:29:30,514 - clustering - INFO - Cluster statistics:\n",
      "2025-09-22 02:29:30,519 - clustering - INFO - \n",
      "        Recency                   Frequency               Monetary            \\\n",
      "           mean     std  min  max      mean  std min max      mean       std   \n",
      "Cluster                                                                        \n",
      "0        177.61  101.48    1  393       1.0  0.0   1   1  53396.86  28500.39   \n",
      "1        526.87  102.03  335  700       1.0  0.0   1   1  51109.73  21080.81   \n",
      "\n",
      "                           \n",
      "            min       max  \n",
      "Cluster                    \n",
      "0        1730.0  666666.0  \n",
      "1        2447.0  160803.0  \n",
      "2025-09-22 02:29:30,521 - clustering - INFO - \n",
      "Cluster sizes:\n",
      "         Size  Percentage\n",
      "Cluster                  \n",
      "1        1135       50.67\n",
      "0        1105       49.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 7: Analyzing clusters...\n"
     ]
    }
   ],
   "source": [
    "# Analyze clusters\n",
    "print(\" STEP 7: Analyzing clusters...\")\n",
    "clustered_data, cluster_stats, cluster_sizes = clustering.analyze_clusters(rfm_df, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1d4b38f-60c8-499f-8772-80e03ec298f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Required files not found. Please ensure `clustering.py` has run correctly.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clustering_results/clustered_customers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load the necessary files created in previous steps\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     clustered_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclustering_results/clustered_customers.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     scaler = joblib.load(\u001b[33m'\u001b[39m\u001b[33mclustering_results/scaler.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded clustered data and scaler.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'clustering_results/clustered_customers.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.clustering import CustomerClustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Change to the project root directory if needed\n",
    "try:\n",
    "    os.chdir(r'C:\\Users\\GCV\\churn_prediction')\n",
    "    print(f\"Working directory changed to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Could not change directory. Please check the path.\")\n",
    "\n",
    "# Create a dummy RFM DataFrame for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "rfm_data = pd.DataFrame({\n",
    "    'Recency': np.random.gamma(2, 20, n_samples),\n",
    "    'Frequency': np.random.poisson(3, n_samples),\n",
    "    'Monetary': np.random.lognormal(3, 1, n_samples)\n",
    "})\n",
    "rfm_data.index.name = 'CustomerID'\n",
    "\n",
    "# Instantiate the clustering class\n",
    "clustering = CustomerClustering(random_state=42)\n",
    "\n",
    "# Prepare and find optimal clusters\n",
    "X_scaled = clustering.prepare_data(rfm_data, scale_features=True)\n",
    "optimal_info = clustering.find_optimal_clusters(X_scaled, max_clusters=8)\n",
    "optimal_k = optimal_info['optimal_k']\n",
    "\n",
    "# Perform clustering and analyze results\n",
    "cluster_labels = clustering.perform_clustering(X_scaled, n_clusters=optimal_k)\n",
    "clustered_data, cluster_stats, cluster_sizes = clustering.analyze_clusters(rfm_data, cluster_labels)\n",
    "\n",
    "# Save the clustering results to the correct output directory\n",
    "clustering.save_clustering_results(clustered_data, output_dir=\"clustering_results\")\n",
    "\n",
    "print(\"✅ Clustering process completed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b354a629-f301-4ae6-8223-a9ce675b51be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\GCV\\churn_prediction\n"
     ]
    }
   ],
   "source": [
    "# Now import the selector\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Change the current working directory to the project's root.\n",
    "# This assumes your notebook is in a subdirectory (e.g., 'notebooks/').\n",
    "# The '../' moves up one directory.\n",
    "# If your notebook is in the root, this line is not needed.\n",
    "os.chdir(r'C:\\Users\\GCV\\churn_prediction')\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Now that we're in the right directory, import the module.\n",
    "from src.model_selection import ModelSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7de5f06-8074-48ca-aedd-ecf7923a443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:49:43,868 - src.model_selection - INFO - Loaded data: 2240 samples, 3 features\n",
      "2025-09-22 00:49:43,874 - src.model_selection - INFO - 🔍 Evaluating RandomForest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:53:31,225 - src.model_selection - INFO - RandomForest best params: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "2025-09-22 00:53:32,219 - src.model_selection - INFO -     RandomForest: AUC = 1.0000 ± 0.0000\n",
      "2025-09-22 00:53:32,220 - src.model_selection - INFO - 🔍 Evaluating XGBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:53:38,437 - src.model_selection - INFO - XGBoost best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "2025-09-22 00:53:38,569 - src.model_selection - INFO -     XGBoost: AUC = 1.0000 ± 0.0000\n",
      "2025-09-22 00:53:38,571 - src.model_selection - INFO - 🔍 Evaluating LogisticRegression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:53:41,516 - src.model_selection - INFO - LogisticRegression best params: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2025-09-22 00:53:41,601 - src.model_selection - INFO -     LogisticRegression: AUC = 1.0000 ± 0.0001\n",
      "2025-09-22 00:53:41,602 - src.model_selection - INFO - 🏆 BEST MODEL: RandomForest (AUC: 1.0000)\n",
      "2025-09-22 00:53:41,672 - src.model_selection - INFO - ✅ Saved RandomForest as best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: RandomForest\n",
      "AUC Score: 1.0000\n",
      "\n",
      "Model: RandomForest\n",
      "  cv_mean: 1.0\n",
      "  cv_std: 0.0\n",
      "  test_auc: 1.0\n",
      "  test_accuracy: 1.0\n",
      "  test_f1: 1.0\n",
      "  test_precision: 1.0\n",
      "  test_recall: 1.0\n",
      "\n",
      "Model: XGBoost\n",
      "  cv_mean: 1.0\n",
      "  cv_std: 0.0\n",
      "  test_auc: 1.0\n",
      "  test_accuracy: 0.875\n",
      "  test_f1: 0.9333333333333333\n",
      "  test_precision: 0.875\n",
      "  test_recall: 1.0\n",
      "\n",
      "Model: LogisticRegression\n",
      "  cv_mean: 0.9999005078973944\n",
      "  cv_std: 0.00010587305099591048\n",
      "  test_auc: 1.0\n",
      "  test_accuracy: 1.0\n",
      "  test_f1: 1.0\n",
      "  test_precision: 1.0\n",
      "  test_recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Initialize the selector\n",
    "selector = ModelSelector(config_path=\"configs/model_config.json\")  # specify your config path\n",
    "\n",
    "# Step 3: Select the best model\n",
    "best_name, best_model, all_results = selector.select_best_model()\n",
    "\n",
    "# Step 4: Save the results and best model\n",
    "selector.save_results()\n",
    "\n",
    "# Step 5: Inspect results\n",
    "print(f\"Best model: {best_name}\")\n",
    "print(f\"AUC Score: {all_results[best_name]['test_auc']:.4f}\")\n",
    "\n",
    "# Optional: See all evaluation metrics\n",
    "for model, metrics in all_results.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'model':\n",
    "            print(f\"  {metric}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad7ad316-d330-457c-b869-a5e47f69d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ STEP 11: Optimizing Random Forest for production...\n",
      "✅ Optimization complete.\n"
     ]
    }
   ],
   "source": [
    "# Correct import statement to get the function\n",
    "from src.model_optimizers.random_forest_optimizer import optimize_random_forest\n",
    "\n",
    "# You no longer need to instantiate a class, as the logic is in a function.\n",
    "# The function requires a model_path and a config dictionary.\n",
    "# You need to define these variables before calling the function.\n",
    "model_path = \"models/Best_Churn_Model.pkl\"  # Example path\n",
    "config = {\n",
    "    \"optimization\": {\n",
    "        \"max_trees_production\": 50  # Example config value\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"⚡ STEP 11: Optimizing Random Forest for production...\")\n",
    "\n",
    "# Correct function call\n",
    "optimized_rf = optimize_random_forest(model_path, config)\n",
    "\n",
    "print(\"✅ Optimization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4416030-a5f0-4999-9640-f200bf435266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:56:30,275 - src.model_selection - INFO - Loaded data: 2240 samples, 3 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model selection and optimization completed!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data using the ModelSelector instance\n",
    "# Assuming 'selector' has already been instantiated. If not, do so first.\n",
    "# from src.model_selection import ModelSelector\n",
    "# selector = ModelSelector(config_path=\"configs/model_config.json\")\n",
    "\n",
    "X, y = selector.load_data()\n",
    "\n",
    "# 2. Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, # Or use the value from your config\n",
    "    random_state=42, # Or use the value from your config\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 3. Now, you can safely evaluate the optimized model\n",
    "optimized_results = selector.evaluate_model(\n",
    "    model=optimized_rf,\n",
    "    model_name='RandomForestOptimized',\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "print(\"✅ Model selection and optimization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a276aca0-8cc2-4617-b48f-1ee31c16844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 00:56:35,340 - src.model_selection - INFO - Loaded data: 2240 samples, 3 features\n",
      "2025-09-22 00:56:35,347 - src.model_selection - INFO - 🔍 Evaluating RandomForest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 01:00:18,393 - src.model_selection - INFO - RandomForest best params: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "2025-09-22 01:00:19,464 - src.model_selection - INFO -     RandomForest: AUC = 1.0000 ± 0.0000\n",
      "2025-09-22 01:00:19,465 - src.model_selection - INFO - 🔍 Evaluating XGBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 01:00:23,756 - src.model_selection - INFO - XGBoost best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "2025-09-22 01:00:23,877 - src.model_selection - INFO -     XGBoost: AUC = 1.0000 ± 0.0000\n",
      "2025-09-22 01:00:23,879 - src.model_selection - INFO - 🔍 Evaluating LogisticRegression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 01:00:26,419 - src.model_selection - INFO - LogisticRegression best params: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "2025-09-22 01:00:26,500 - src.model_selection - INFO -     LogisticRegression: AUC = 1.0000 ± 0.0001\n",
      "2025-09-22 01:00:26,502 - src.model_selection - INFO - 🏆 BEST MODEL: RandomForest (AUC: 1.0000)\n",
      "2025-09-22 01:00:26,511 - src.model_selection - INFO - Loaded data: 2240 samples, 3 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL PERFORMANCE ===\n",
      "\n",
      "RANDOMFOREST:\n",
      " Accuracy: 1.0000\n",
      " Precision: 1.0000\n",
      " Recall: 1.0000\n",
      " F1-Score: 1.0000\n",
      " ROC-AUC: 1.0000\n",
      " Top Features:\n",
      "   - Recency: 0.9794\n",
      "   - Monetary: 0.0206\n",
      "   - Frequency: 0.0000\n",
      "\n",
      "XGBOOST:\n",
      " Accuracy: 0.8750\n",
      " Precision: 0.8750\n",
      " Recall: 1.0000\n",
      " F1-Score: 0.9333\n",
      " ROC-AUC: 1.0000\n",
      "\n",
      "LOGISTICREGRESSION:\n",
      " Accuracy: 1.0000\n",
      " Precision: 1.0000\n",
      " Recall: 1.0000\n",
      " F1-Score: 1.0000\n",
      " ROC-AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the necessary class\n",
    "from src.model_selection import ModelSelector\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Instantiate the ModelSelector class\n",
    "selector = ModelSelector(config_path=\"configs/model_config.json\")\n",
    "\n",
    "# 3. Call the method to get the results and assign them to a variable\n",
    "best_model_name, best_model, all_results = selector.select_best_model()\n",
    "\n",
    "# 4. Get feature names from the loaded data for the feature importance display\n",
    "X, _ = selector.load_data()\n",
    "feature_names = X.columns\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE ===\")\n",
    "for model_name, metrics in all_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    # Use the correct keys from the metrics dictionary\n",
    "    print(f\" Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\" Precision: {metrics['test_precision']:.4f}\")\n",
    "    print(f\" Recall: {metrics['test_recall']:.4f}\")\n",
    "    print(f\" F1-Score: {metrics['test_f1']:.4f}\")\n",
    "    print(f\" ROC-AUC: {metrics['test_auc']:.4f}\")\n",
    "    \n",
    "    # Show feature importance for relevant models\n",
    "    if 'RandomForest' in model_name:\n",
    "        # Check if the model has feature_importances_ and if feature_names are defined\n",
    "        if hasattr(metrics.get('model'), 'feature_importances_') and feature_names is not None:\n",
    "            # Create a list of (feature, importance) tuples and sort by importance\n",
    "            feature_importances = list(zip(feature_names, metrics['model'].feature_importances_))\n",
    "            feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "            print(\" Top Features:\")\n",
    "            for feature, importance in feature_importances[:3]:\n",
    "                print(f\"   - {feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d8781a6-bb36-42b9-bfac-f189fa425c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 STEP 12: Generating business insights...\n"
     ]
    }
   ],
   "source": [
    "# Generate business insights\n",
    "from src.insights.business_insights import BusinessInsights\n",
    "print(\"💡 STEP 12: Generating business insights...\")\n",
    "insights = BusinessInsights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56977991-fc10-41eb-b078-f79f315f2fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:30:54,149 - src.insights.business_insights - INFO - Generating insights for RandomForest model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory changed to: C:\\Users\\GCV\\churn_prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:30:54,515 - src.insights.business_insights - INFO - Feature importance plot saved to output/feature_importance.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot has been generated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change to the project root directory\n",
    "try:\n",
    "    os.chdir(r'C:\\Users\\GCV\\churn_prediction')\n",
    "    print(f\"Working directory changed to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Could not change directory. Please check the path.\")\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import the necessary class\n",
    "from src.insights.business_insights import BusinessInsights\n",
    "\n",
    "# Instantiate the BusinessInsights class\n",
    "insights = BusinessInsights()\n",
    "\n",
    "# Load the model information\n",
    "insights.load_model_info()\n",
    "\n",
    "# Calculate and get the feature importance DataFrame\n",
    "feature_importance_df = insights.calculate_feature_importance()\n",
    "\n",
    "# Plot the feature importance and save the file\n",
    "insights.plot_feature_importance(feature_importance_df, 'output/feature_importance.png')\n",
    "\n",
    "# Display a message to confirm the plot was created\n",
    "print(\"Feature importance plot has been generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "832e839d-ba09-458e-9f9d-c670ade92103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 02:31:01,659 - src.model_selection - INFO - Loaded data: 2240 samples, 3 features\n",
      "2025-09-22 02:31:01,831 - src.insights.business_insights - INFO - Generating insights for RandomForest model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory changed to: C:\\Users\\GCV\\churn_prediction\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BusinessInsights' object has no attribute 'analyze_segments'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     37\u001b[39m insights.load_model_info()\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 5. Analyze the segments using the new method\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 'optimized_rf' and 'X' should be defined in a previous cell\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 'X' here is used as the features for the model, as per your code\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# You'll need to make sure 'optimized_rf' is available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m segmentation_insights = \u001b[43minsights\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_segments\u001b[49m(clustered_data, insights.model, feature_names)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Customer Segmentation Insights ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'BusinessInsights' object has no attribute 'analyze_segments'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from src.insights.business_insights import BusinessInsights\n",
    "from src.model_selection import ModelSelector\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Change to the project root directory\n",
    "try:\n",
    "    os.chdir(r'C:\\Users\\GCV\\churn_prediction')\n",
    "    print(f\"Working directory changed to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Could not change directory. Please check the path.\")\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# 1. Load Data\n",
    "selector = ModelSelector(config_path=\"configs/model_config.json\")\n",
    "X, y = selector.load_data()\n",
    "X = X.reset_index(drop=True)\n",
    "customer_ids = X.index.values # Assuming the index is the customer_id\n",
    "feature_names = ['Recency', 'Frequency', 'Monetary']\n",
    "X_features = X[feature_names]\n",
    "\n",
    "# 2. Perform K-Means Clustering for Segmentation\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans.fit(X_features)\n",
    "X_features['segment'] = kmeans.labels_\n",
    "\n",
    "# 3. Prepare data for analysis\n",
    "clustered_data = X_features.copy()\n",
    "clustered_data['customer_id'] = customer_ids\n",
    "\n",
    "# 4. Instantiate BusinessInsights and load the model\n",
    "insights = BusinessInsights()\n",
    "insights.load_model_info()\n",
    "\n",
    "# 5. Analyze the segments using the new method\n",
    "# 'optimized_rf' and 'X' should be defined in a previous cell\n",
    "# 'X' here is used as the features for the model, as per your code\n",
    "# You'll need to make sure 'optimized_rf' is available\n",
    "segmentation_insights = insights.analyze_segments(clustered_data, insights.model, feature_names)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\n=== Customer Segmentation Insights ===\")\n",
    "print(segmentation_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97897cb-21c2-489a-973f-1dd899ff67d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54e6ad-14a4-4e07-a24d-f9212edd6cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
